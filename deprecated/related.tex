\section{Related Works}

Our choice of using publication as the basic unit to measure the impact is backed up by the fact that the bibliometric based criteria are the de-facto standards to measure the impact of research. For example, publication derived metrics are broadly used in Faculty recruit/promotion, institution ranking \cite{thomas1998institutional}.

For a publication, while usage based metrics are proposed by some research works \cite{Bollen:2007:MUM:1255175.1255273} \cite{Bollen:2008:TUI:1378889.1378928} \cite{bollen2009principal}, citation based metrics are probably still the standard and well accepted measure. In addition to the intuitive measures like number of publications and number of citations, h-index \cite{hirsch2005index} and g-index \cite{egghe2006theory} are other two popular metrics. Publication counts could be seen as a measure of the productivity, while citation counts measure the quality, or impact of the work published. As h-index and g-index calculate the metric by combining these data, they measure both the productivity and the quality, thus impact in general. For instance, nanoHub used these criteria to measure the impact of their project \cite{www-nanohubcite}. It defined a `citation' as a published work that cites/refers nanohub site or its related content, and `Secondary citation' as the citation received of the previously defined `citation'. Based on this, it analyzed the statistical distributions of the `citation' related data based on different criteria, e.g., author's organization; topic area - research, education; cited year; publication type; etc. The overall secondary citation and h-index are also computed to show the overall impact of the project.

There are existing tools to measure the metrics for individual users, e.g. Scholarometer \cite{kaur2012scholarometer} and Publish or Perish \cite{www-pop}. These could be potentially leveraged to analyze a relatively small group of users, e.g., the work \cite{bollen2011and} showing TeraGrid's impact based on limited data from one resource allocation meeting consisting of 112 PIs.
However, neither of the tools provides a scalable solution to the large community we are concerning which consists of over 20 thousands users.

While a more formal publication based metrics, either based on citation or usage, are still the most prevailing criteria, there are other proposals to include other measures. E.g., altmetrics \cite{www-altmetrics} proposes to include measures for dataset, code; as well as mentioning of a snippet of work via social networking; among others. We acknowledge these efforts as the trend of big data and social networking might suggest, however at the present there still lacks a standard and well established way to objectively derive measures based on these means.

